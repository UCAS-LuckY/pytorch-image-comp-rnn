{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compression_rnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UCAS-LuckY/pytorch-image-comp-rnn/blob/master/compression_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N-9Vvdp1LYg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2a1ecb64-b2ab-4a2d-87e2-7d440abdcc31"
      },
      "source": [
        "cd /content/drive/My Drive/Colab Notebooks/pytorch-image-comp-rnn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/pytorch-image-comp-rnn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzAgHNrL1RXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "be83f648-284f-41e3-b19e-f4fd128b9256"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8ZKLoMq1i6R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6b0c9fd-dc8d-4096-95f0-be6303faa8cc"
      },
      "source": [
        "!python train.py -f flower_photos/daisy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total images: 633; total batches: 20\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "[TRAIN] Epoch[1](1/20); Loss: 0.293414; Backpropagation: 0.4954 sec; Batch: 3.6029 sec\n",
            "0.2934 0.2934 0.2934 0.2934 0.2934 0.2934 0.2934 0.2934 0.2934 0.2934 0.2934 0.2934 0.2934 0.2934 0.2934 0.2934 \n",
            "\n",
            "[TRAIN] Epoch[1](2/20); Loss: 0.282940; Backpropagation: 0.3593 sec; Batch: 3.4338 sec\n",
            "0.2830 0.2830 0.2830 0.2830 0.2830 0.2829 0.2829 0.2829 0.2829 0.2829 0.2829 0.2829 0.2829 0.2829 0.2829 0.2829 \n",
            "\n",
            "[TRAIN] Epoch[1](3/20); Loss: 0.301356; Backpropagation: 0.3633 sec; Batch: 3.4495 sec\n",
            "0.3017 0.3017 0.3017 0.3017 0.3016 0.3016 0.3016 0.3015 0.3014 0.3013 0.3012 0.3011 0.3010 0.3009 0.3008 0.3007 \n",
            "\n",
            "[TRAIN] Epoch[1](4/20); Loss: 0.267519; Backpropagation: 0.3601 sec; Batch: 3.4498 sec\n",
            "0.2707 0.2707 0.2707 0.2705 0.2703 0.2700 0.2696 0.2690 0.2684 0.2676 0.2667 0.2657 0.2646 0.2633 0.2620 0.2605 \n",
            "\n",
            "[TRAIN] Epoch[1](5/20); Loss: 0.268674; Backpropagation: 0.3645 sec; Batch: 3.4628 sec\n",
            "0.3128 0.3127 0.3122 0.3111 0.3090 0.3054 0.2998 0.2918 0.2811 0.2677 0.2519 0.2342 0.2167 0.2026 0.1948 0.1949 \n",
            "\n",
            "[TRAIN] Epoch[1](6/20); Loss: 0.366783; Backpropagation: 0.3701 sec; Batch: 3.4918 sec\n",
            "0.2923 0.2913 0.2883 0.2810 0.2666 0.2428 0.2109 0.1828 0.1872 0.2285 0.2963 0.3858 0.4926 0.6117 0.7389 0.8715 \n",
            "\n",
            "[TRAIN] Epoch[1](7/20); Loss: 0.256063; Backpropagation: 0.3650 sec; Batch: 3.4686 sec\n",
            "0.2835 0.2832 0.2825 0.2807 0.2775 0.2724 0.2651 0.2555 0.2443 0.2326 0.2222 0.2171 0.2203 0.2320 0.2512 0.2769 \n",
            "\n",
            "[TRAIN] Epoch[1](8/20); Loss: 0.264835; Backpropagation: 0.3614 sec; Batch: 3.4958 sec\n",
            "0.2926 0.2924 0.2919 0.2910 0.2894 0.2870 0.2835 0.2789 0.2729 0.2657 0.2573 0.2478 0.2375 0.2268 0.2163 0.2064 \n",
            "\n",
            "[TRAIN] Epoch[1](9/20); Loss: 0.276845; Backpropagation: 0.4279 sec; Batch: 3.5315 sec\n",
            "0.2915 0.2914 0.2911 0.2905 0.2895 0.2881 0.2862 0.2837 0.2806 0.2769 0.2727 0.2681 0.2631 0.2577 0.2521 0.2464 \n",
            "\n",
            "[TRAIN] Epoch[1](10/20); Loss: 0.270622; Backpropagation: 0.3623 sec; Batch: 3.4562 sec\n",
            "0.2858 0.2857 0.2855 0.2851 0.2843 0.2830 0.2811 0.2785 0.2752 0.2713 0.2668 0.2616 0.2559 0.2498 0.2434 0.2368 \n",
            "\n",
            "[TRAIN] Epoch[1](11/20); Loss: 0.257350; Backpropagation: 0.3630 sec; Batch: 3.4593 sec\n",
            "0.2708 0.2708 0.2706 0.2702 0.2694 0.2682 0.2664 0.2639 0.2609 0.2573 0.2531 0.2485 0.2437 0.2389 0.2344 0.2304 \n",
            "\n",
            "[TRAIN] Epoch[1](12/20); Loss: 0.281045; Backpropagation: 0.3608 sec; Batch: 3.4494 sec\n",
            "0.3013 0.3012 0.3010 0.3003 0.2989 0.2966 0.2930 0.2881 0.2818 0.2741 0.2659 0.2589 0.2548 0.2547 0.2590 0.2672 \n",
            "\n",
            "[TRAIN] Epoch[1](13/20); Loss: 0.260726; Backpropagation: 0.3689 sec; Batch: 3.5084 sec\n",
            "0.2810 0.2807 0.2802 0.2791 0.2771 0.2739 0.2692 0.2628 0.2546 0.2452 0.2368 0.2326 0.2343 0.2414 0.2534 0.2695 \n",
            "\n",
            "[TRAIN] Epoch[1](14/20); Loss: 0.252534; Backpropagation: 0.3619 sec; Batch: 3.4541 sec\n",
            "0.2774 0.2770 0.2762 0.2745 0.2720 0.2683 0.2635 0.2575 0.2504 0.2427 0.2354 0.2301 0.2274 0.2271 0.2288 0.2320 \n",
            "\n",
            "[TRAIN] Epoch[1](15/20); Loss: 0.244640; Backpropagation: 0.3613 sec; Batch: 3.4510 sec\n",
            "0.2798 0.2794 0.2782 0.2758 0.2716 0.2652 0.2563 0.2451 0.2319 0.2182 0.2070 0.2024 0.2057 0.2158 0.2313 0.2504 \n",
            "\n",
            "[TRAIN] Epoch[1](16/20); Loss: 0.236592; Backpropagation: 0.3634 sec; Batch: 3.4542 sec\n",
            "0.2775 0.2769 0.2755 0.2726 0.2680 0.2613 0.2528 0.2428 0.2319 0.2207 0.2103 0.2019 0.1967 0.1956 0.1979 0.2030 \n",
            "\n",
            "[TRAIN] Epoch[1](17/20); Loss: 0.215883; Backpropagation: 0.3636 sec; Batch: 3.4638 sec\n",
            "0.2685 0.2679 0.2661 0.2626 0.2568 0.2485 0.2379 0.2253 0.2117 0.1981 0.1856 0.1748 0.1666 0.1617 0.1602 0.1618 \n",
            "\n",
            "[TRAIN] Epoch[1](18/20); Loss: 0.229175; Backpropagation: 0.3628 sec; Batch: 3.4573 sec\n",
            "0.2792 0.2783 0.2760 0.2715 0.2646 0.2556 0.2448 0.2327 0.2203 0.2083 0.1977 0.1892 0.1840 0.1832 0.1870 0.1945 \n",
            "\n",
            "[TRAIN] Epoch[1](19/20); Loss: 0.217999; Backpropagation: 0.3593 sec; Batch: 3.4512 sec\n",
            "0.2753 0.2732 0.2681 0.2591 0.2462 0.2306 0.2134 0.1963 0.1813 0.1708 0.1668 0.1705 0.1813 0.1975 0.2177 0.2400 \n",
            "\n",
            "[TRAIN] Epoch[1](20/20); Loss: 0.224011; Backpropagation: 0.3358 sec; Batch: 2.8372 sec\n",
            "0.2653 0.2631 0.2580 0.2494 0.2377 0.2245 0.2111 0.1989 0.1893 0.1836 0.1835 0.1905 0.2039 0.2217 0.2416 0.2620 \n",
            "\n",
            "[TRAIN] Epoch[2](1/20); Loss: 0.229203; Backpropagation: 0.3671 sec; Batch: 3.4623 sec\n",
            "0.3058 0.3026 0.2954 0.2838 0.2685 0.2507 0.2319 0.2135 0.1967 0.1824 0.1729 0.1709 0.1767 0.1886 0.2045 0.2224 \n",
            "\n",
            "[TRAIN] Epoch[2](2/20); Loss: 0.225563; Backpropagation: 0.3742 sec; Batch: 3.4687 sec\n",
            "0.2913 0.2878 0.2804 0.2693 0.2556 0.2404 0.2251 0.2107 0.1980 0.1879 0.1814 0.1796 0.1835 0.1928 0.2056 0.2197 \n",
            "\n",
            "[TRAIN] Epoch[2](3/20); Loss: 0.205012; Backpropagation: 0.3720 sec; Batch: 3.4639 sec\n",
            "0.2702 0.2664 0.2585 0.2468 0.2325 0.2173 0.2022 0.1886 0.1775 0.1698 0.1660 0.1661 0.1695 0.1754 0.1827 0.1906 \n",
            "\n",
            "[TRAIN] Epoch[2](4/20); Loss: 0.209096; Backpropagation: 0.3690 sec; Batch: 3.4717 sec\n",
            "0.2967 0.2911 0.2805 0.2658 0.2487 0.2310 0.2140 0.1983 0.1847 0.1737 0.1653 0.1596 0.1565 0.1562 0.1590 0.1645 \n",
            "\n",
            "[TRAIN] Epoch[2](5/20); Loss: 0.210506; Backpropagation: 0.3677 sec; Batch: 3.5285 sec\n",
            "0.2835 0.2787 0.2698 0.2576 0.2433 0.2282 0.2137 0.2005 0.1891 0.1800 0.1733 0.1692 0.1676 0.1682 0.1708 0.1745 \n",
            "\n",
            "[TRAIN] Epoch[2](6/20); Loss: 0.187908; Backpropagation: 0.3681 sec; Batch: 3.4624 sec\n",
            "0.2639 0.2567 0.2447 0.2294 0.2131 0.1978 0.1846 0.1737 0.1654 0.1592 0.1552 0.1532 0.1524 0.1522 0.1523 0.1529 \n",
            "\n",
            "[TRAIN] Epoch[2](7/20); Loss: 0.202241; Backpropagation: 0.3715 sec; Batch: 3.5134 sec\n",
            "0.2768 0.2696 0.2580 0.2439 0.2288 0.2140 0.2006 0.1891 0.1799 0.1730 0.1682 0.1654 0.1643 0.1651 0.1677 0.1716 \n",
            "\n",
            "[TRAIN] Epoch[2](8/20); Loss: 0.195529; Backpropagation: 0.3673 sec; Batch: 3.4640 sec\n",
            "0.2756 0.2677 0.2553 0.2400 0.2239 0.2086 0.1949 0.1834 0.1742 0.1671 0.1619 0.1584 0.1560 0.1544 0.1536 0.1535 \n",
            "\n",
            "[TRAIN] Epoch[2](9/20); Loss: 0.198397; Backpropagation: 0.3714 sec; Batch: 3.4671 sec\n",
            "0.2733 0.2647 0.2518 0.2367 0.2217 0.2080 0.1963 0.1870 0.1795 0.1733 0.1682 0.1645 0.1624 0.1618 0.1621 0.1632 \n",
            "\n",
            "[TRAIN] Epoch[2](10/20); Loss: 0.176081; Backpropagation: 0.4663 sec; Batch: 3.5591 sec\n",
            "0.2668 0.2553 0.2387 0.2196 0.2011 0.1855 0.1739 0.1652 0.1578 0.1504 0.1432 0.1372 0.1328 0.1301 0.1292 0.1305 \n",
            "\n",
            "[TRAIN] Epoch[2](11/20); Loss: 0.199334; Backpropagation: 0.3680 sec; Batch: 3.5083 sec\n",
            "0.3021 0.2883 0.2695 0.2485 0.2281 0.2098 0.1945 0.1818 0.1710 0.1612 0.1530 0.1481 0.1485 0.1540 0.1617 0.1691 \n",
            "\n",
            "[TRAIN] Epoch[2](12/20); Loss: 0.172938; Backpropagation: 0.3675 sec; Batch: 3.5838 sec\n",
            "0.2648 0.2491 0.2272 0.2042 0.1842 0.1690 0.1586 0.1515 0.1463 0.1421 0.1394 0.1389 0.1407 0.1447 0.1498 0.1564 \n",
            "\n",
            "[TRAIN] Epoch[2](13/20); Loss: 0.185348; Backpropagation: 0.3690 sec; Batch: 3.4668 sec\n",
            "0.2933 0.2763 0.2524 0.2262 0.2025 0.1839 0.1712 0.1629 0.1569 0.1513 0.1449 0.1398 0.1388 0.1442 0.1547 0.1663 \n",
            "\n",
            "[TRAIN] Epoch[2](14/20); Loss: 0.175497; Backpropagation: 0.3665 sec; Batch: 3.4630 sec\n",
            "0.2927 0.2720 0.2455 0.2196 0.1976 0.1792 0.1629 0.1481 0.1354 0.1279 0.1276 0.1322 0.1379 0.1412 0.1425 0.1457 \n",
            "\n",
            "[TRAIN] Epoch[2](15/20); Loss: 0.160332; Backpropagation: 0.3681 sec; Batch: 3.4695 sec\n",
            "0.2667 0.2452 0.2168 0.1905 0.1706 0.1561 0.1442 0.1330 0.1240 0.1207 0.1235 0.1304 0.1370 0.1379 0.1346 0.1341 \n",
            "\n",
            "[TRAIN] Epoch[2](16/20); Loss: 0.171596; Backpropagation: 0.3680 sec; Batch: 3.4640 sec\n",
            "0.2844 0.2597 0.2305 0.2037 0.1822 0.1657 0.1527 0.1427 0.1355 0.1315 0.1319 0.1370 0.1433 0.1459 0.1480 0.1508 \n",
            "\n",
            "[TRAIN] Epoch[2](17/20); Loss: 0.155554; Backpropagation: 0.4680 sec; Batch: 3.5638 sec\n",
            "0.2480 0.2254 0.2015 0.1813 0.1654 0.1521 0.1401 0.1310 0.1262 0.1268 0.1301 0.1329 0.1325 0.1306 0.1316 0.1334 \n",
            "\n",
            "[TRAIN] Epoch[2](18/20); Loss: 0.170733; Backpropagation: 0.3680 sec; Batch: 3.5133 sec\n",
            "0.3023 0.2723 0.2396 0.2106 0.1869 0.1673 0.1500 0.1350 0.1275 0.1274 0.1296 0.1320 0.1350 0.1385 0.1397 0.1381 \n",
            "\n",
            "[TRAIN] Epoch[2](19/20); Loss: 0.166408; Backpropagation: 0.3707 sec; Batch: 3.4755 sec\n",
            "0.2871 0.2583 0.2272 0.2016 0.1806 0.1617 0.1459 0.1354 0.1318 0.1323 0.1331 0.1330 0.1335 0.1349 0.1340 0.1320 \n",
            "\n",
            "[TRAIN] Epoch[2](20/20); Loss: 0.156568; Backpropagation: 0.3364 sec; Batch: 2.9121 sec\n",
            "0.2795 0.2466 0.2124 0.1850 0.1646 0.1484 0.1354 0.1259 0.1221 0.1221 0.1222 0.1230 0.1250 0.1273 0.1306 0.1350 \n",
            "\n",
            "[TRAIN] Epoch[3](1/20); Loss: 0.154781; Backpropagation: 0.3675 sec; Batch: 3.4677 sec\n",
            "0.2488 0.2175 0.1888 0.1672 0.1524 0.1412 0.1331 0.1310 0.1332 0.1350 0.1350 0.1351 0.1367 0.1398 0.1412 0.1406 \n",
            "\n",
            "[TRAIN] Epoch[3](2/20); Loss: 0.136434; Backpropagation: 0.3693 sec; Batch: 3.5128 sec\n",
            "0.2537 0.2172 0.1776 0.1461 0.1273 0.1189 0.1154 0.1131 0.1116 0.1118 0.1123 0.1108 0.1100 0.1136 0.1194 0.1242 \n",
            "\n",
            "[TRAIN] Epoch[3](3/20); Loss: 0.141795; Backpropagation: 0.3687 sec; Batch: 3.4666 sec\n",
            "0.2445 0.2110 0.1772 0.1512 0.1342 0.1250 0.1216 0.1212 0.1215 0.1238 0.1249 0.1220 0.1213 0.1230 0.1229 0.1235 \n",
            "\n",
            "[TRAIN] Epoch[3](4/20); Loss: 0.141316; Backpropagation: 0.3701 sec; Batch: 3.4656 sec\n",
            "0.2437 0.2111 0.1792 0.1561 0.1409 0.1301 0.1233 0.1183 0.1154 0.1154 0.1162 0.1184 0.1211 0.1236 0.1245 0.1239 \n",
            "\n",
            "[TRAIN] Epoch[3](5/20); Loss: 0.143997; Backpropagation: 0.3681 sec; Batch: 3.4734 sec\n",
            "0.2693 0.2314 0.1944 0.1659 0.1444 0.1285 0.1204 0.1176 0.1156 0.1135 0.1133 0.1146 0.1159 0.1185 0.1198 0.1208 \n",
            "\n",
            "[TRAIN] Epoch[3](6/20); Loss: 0.153278; Backpropagation: 0.3676 sec; Batch: 3.5107 sec\n",
            "0.2789 0.2424 0.2045 0.1728 0.1486 0.1337 0.1279 0.1262 0.1254 0.1240 0.1238 0.1257 0.1283 0.1300 0.1300 0.1301 \n",
            "\n",
            "[TRAIN] Epoch[3](7/20); Loss: 0.149101; Backpropagation: 0.3687 sec; Batch: 3.4684 sec\n",
            "0.2761 0.2372 0.1976 0.1640 0.1401 0.1260 0.1207 0.1209 0.1221 0.1229 0.1241 0.1250 0.1256 0.1263 0.1281 0.1290 \n",
            "\n",
            "[TRAIN] Epoch[3](8/20); Loss: 0.147456; Backpropagation: 0.3702 sec; Batch: 3.4675 sec\n",
            "0.2357 0.2039 0.1756 0.1549 0.1407 0.1320 0.1298 0.1305 0.1309 0.1308 0.1309 0.1318 0.1318 0.1322 0.1335 0.1341 \n",
            "\n",
            "[TRAIN] Epoch[3](9/20); Loss: 0.149840; Backpropagation: 0.3691 sec; Batch: 3.4752 sec\n",
            "0.2702 0.2301 0.1933 0.1651 0.1452 0.1329 0.1270 0.1243 0.1225 0.1231 0.1240 0.1251 0.1271 0.1294 0.1296 0.1284 \n",
            "\n",
            "[TRAIN] Epoch[3](10/20); Loss: 0.138284; Backpropagation: 0.3692 sec; Batch: 3.4633 sec\n",
            "0.2539 0.2161 0.1797 0.1523 0.1316 0.1192 0.1150 0.1142 0.1130 0.1126 0.1153 0.1187 0.1186 0.1165 0.1166 0.1193 \n",
            "\n",
            "[TRAIN] Epoch[3](11/20); Loss: 0.137366; Backpropagation: 0.3670 sec; Batch: 3.4655 sec\n",
            "0.2390 0.2021 0.1679 0.1456 0.1339 0.1255 0.1209 0.1193 0.1185 0.1175 0.1177 0.1183 0.1188 0.1186 0.1169 0.1173 \n",
            "\n",
            "[TRAIN] Epoch[3](12/20); Loss: 0.133693; Backpropagation: 0.3690 sec; Batch: 3.5175 sec\n",
            "0.2664 0.2223 0.1797 0.1481 0.1260 0.1130 0.1089 0.1083 0.1083 0.1073 0.1071 0.1100 0.1119 0.1097 0.1064 0.1057 \n",
            "\n",
            "[TRAIN] Epoch[3](13/20); Loss: 0.139394; Backpropagation: 0.3672 sec; Batch: 3.4670 sec\n",
            "0.2559 0.2155 0.1776 0.1498 0.1305 0.1201 0.1177 0.1174 0.1172 0.1170 0.1169 0.1188 0.1198 0.1180 0.1183 0.1197 \n",
            "\n",
            "[TRAIN] Epoch[3](14/20); Loss: 0.151233; Backpropagation: 0.3695 sec; Batch: 3.4713 sec\n",
            "0.2555 0.2162 0.1822 0.1591 0.1444 0.1346 0.1308 0.1314 0.1322 0.1320 0.1341 0.1353 0.1349 0.1328 0.1320 0.1322 \n",
            "\n",
            "[TRAIN] Epoch[3](15/20); Loss: 0.129716; Backpropagation: 0.3675 sec; Batch: 3.4621 sec\n",
            "0.2360 0.1973 0.1606 0.1348 0.1190 0.1110 0.1083 0.1093 0.1093 0.1099 0.1118 0.1121 0.1141 0.1153 0.1137 0.1128 \n",
            "\n",
            "[TRAIN] Epoch[3](16/20); Loss: 0.135813; Backpropagation: 0.3687 sec; Batch: 3.4662 sec\n",
            "0.2332 0.1897 0.1551 0.1348 0.1243 0.1203 0.1204 0.1213 0.1208 0.1206 0.1209 0.1205 0.1206 0.1229 0.1241 0.1235 \n",
            "\n",
            "[TRAIN] Epoch[3](17/20); Loss: 0.139780; Backpropagation: 0.3699 sec; Batch: 3.4689 sec\n",
            "0.2865 0.2322 0.1817 0.1467 0.1259 0.1162 0.1124 0.1120 0.1121 0.1132 0.1149 0.1161 0.1159 0.1158 0.1171 0.1177 \n",
            "\n",
            "[TRAIN] Epoch[3](18/20); Loss: 0.131647; Backpropagation: 0.3705 sec; Batch: 3.5271 sec\n",
            "0.2597 0.2099 0.1628 0.1298 0.1131 0.1102 0.1107 0.1105 0.1104 0.1111 0.1126 0.1132 0.1123 0.1131 0.1139 0.1130 \n",
            "\n",
            "[TRAIN] Epoch[3](19/20); Loss: 0.135640; Backpropagation: 0.3652 sec; Batch: 3.4653 sec\n",
            "0.2401 0.1993 0.1658 0.1416 0.1268 0.1209 0.1185 0.1178 0.1167 0.1170 0.1183 0.1183 0.1176 0.1167 0.1170 0.1177 \n",
            "\n",
            "[TRAIN] Epoch[3](20/20); Loss: 0.137081; Backpropagation: 0.3361 sec; Batch: 2.9126 sec\n",
            "0.2202 0.1863 0.1590 0.1380 0.1270 0.1235 0.1236 0.1225 0.1215 0.1233 0.1246 0.1235 0.1233 0.1242 0.1257 0.1272 \n",
            "\n",
            "[TRAIN] Epoch[4](1/20); Loss: 0.133262; Backpropagation: 0.3671 sec; Batch: 3.4638 sec\n",
            "0.2243 0.1832 0.1498 0.1293 0.1219 0.1210 0.1217 0.1218 0.1197 0.1190 0.1189 0.1199 0.1214 0.1208 0.1198 0.1198 \n",
            "\n",
            "[TRAIN] Epoch[4](2/20); Loss: 0.141708; Backpropagation: 0.3737 sec; Batch: 3.4740 sec\n",
            "0.2580 0.2112 0.1699 0.1443 0.1312 0.1270 0.1247 0.1239 0.1230 0.1224 0.1221 0.1229 0.1236 0.1222 0.1210 0.1202 \n",
            "\n",
            "[TRAIN] Epoch[4](3/20); Loss: 0.117816; Backpropagation: 0.3684 sec; Batch: 3.4647 sec\n",
            "0.2611 0.2009 0.1518 0.1209 0.1020 0.0976 0.0976 0.0965 0.0940 0.0924 0.0944 0.0948 0.0934 0.0943 0.0963 0.0971 \n",
            "\n",
            "[TRAIN] Epoch[4](4/20); Loss: 0.131293; Backpropagation: 0.3696 sec; Batch: 3.5213 sec\n",
            "0.2510 0.2000 0.1572 0.1319 0.1202 0.1163 0.1137 0.1135 0.1139 0.1135 0.1113 0.1106 0.1114 0.1121 0.1129 0.1112 \n",
            "\n",
            "[TRAIN] Epoch[4](5/20); Loss: 0.127885; Backpropagation: 0.3691 sec; Batch: 3.4708 sec\n",
            "0.2483 0.1959 0.1561 0.1274 0.1141 0.1130 0.1108 0.1092 0.1088 0.1104 0.1100 0.1087 0.1079 0.1085 0.1080 0.1090 \n",
            "\n",
            "[TRAIN] Epoch[4](6/20); Loss: 0.133496; Backpropagation: 0.3670 sec; Batch: 3.4619 sec\n",
            "0.2296 0.1875 0.1541 0.1319 0.1238 0.1208 0.1187 0.1175 0.1175 0.1196 0.1198 0.1189 0.1189 0.1190 0.1193 0.1193 \n",
            "\n",
            "[TRAIN] Epoch[4](7/20); Loss: 0.129323; Backpropagation: 0.3694 sec; Batch: 3.4760 sec\n",
            "0.2490 0.1917 0.1464 0.1236 0.1169 0.1146 0.1131 0.1112 0.1120 0.1124 0.1130 0.1129 0.1122 0.1120 0.1139 0.1140 \n",
            "\n",
            "[TRAIN] Epoch[4](8/20); Loss: 0.147501; Backpropagation: 0.3701 sec; Batch: 3.4725 sec\n",
            "0.2738 0.2173 0.1693 0.1435 0.1354 0.1313 0.1293 0.1294 0.1293 0.1289 0.1291 0.1295 0.1285 0.1285 0.1286 0.1284 \n",
            "\n",
            "[TRAIN] Epoch[4](9/20); Loss: 0.147781; Backpropagation: 0.3702 sec; Batch: 3.5213 sec\n",
            "0.2488 0.2019 0.1677 0.1478 0.1402 0.1377 0.1349 0.1324 0.1318 0.1321 0.1320 0.1305 0.1300 0.1320 0.1323 0.1326 \n",
            "\n",
            "[TRAIN] Epoch[4](10/20); Loss: 0.126002; Backpropagation: 0.3698 sec; Batch: 3.4655 sec\n",
            "0.2142 0.1631 0.1324 0.1182 0.1145 0.1130 0.1137 0.1138 0.1140 0.1152 0.1149 0.1150 0.1163 0.1181 0.1198 0.1197 \n",
            "\n",
            "[TRAIN] Epoch[4](11/20); Loss: 0.111055; Backpropagation: 0.4672 sec; Batch: 3.5593 sec\n",
            "0.2482 0.1769 0.1267 0.0965 0.0916 0.0913 0.0912 0.0895 0.0915 0.0947 0.0943 0.0944 0.0955 0.0973 0.0986 0.0985 \n",
            "\n",
            "[TRAIN] Epoch[4](12/20); Loss: 0.111299; Backpropagation: 0.3727 sec; Batch: 3.4752 sec\n",
            "0.2351 0.1789 0.1310 0.1042 0.0965 0.0950 0.0939 0.0934 0.0937 0.0935 0.0940 0.0939 0.0936 0.0943 0.0948 0.0951 \n",
            "\n",
            "[TRAIN] Epoch[4](13/20); Loss: 0.123065; Backpropagation: 0.4662 sec; Batch: 3.5638 sec\n",
            "0.2307 0.1783 0.1427 0.1200 0.1134 0.1107 0.1090 0.1074 0.1084 0.1081 0.1069 0.1059 0.1063 0.1062 0.1076 0.1073 \n",
            "\n",
            "[TRAIN] Epoch[4](14/20); Loss: 0.112238; Backpropagation: 0.3709 sec; Batch: 3.4665 sec\n",
            "0.2078 0.1572 0.1250 0.1117 0.1063 0.1037 0.1022 0.1003 0.0988 0.0984 0.0971 0.0979 0.0992 0.0970 0.0970 0.0963 \n",
            "\n",
            "[TRAIN] Epoch[4](15/20); Loss: 0.127711; Backpropagation: 0.3697 sec; Batch: 3.4708 sec\n",
            "0.2193 0.1689 0.1365 0.1256 0.1231 0.1197 0.1176 0.1170 0.1168 0.1152 0.1143 0.1146 0.1136 0.1134 0.1141 0.1137 \n",
            "\n",
            "[TRAIN] Epoch[4](16/20); Loss: 0.117843; Backpropagation: 0.3678 sec; Batch: 3.4718 sec\n",
            "0.2390 0.1763 0.1325 0.1114 0.1065 0.1033 0.1026 0.1021 0.1020 0.1013 0.1011 0.1011 0.1010 0.1015 0.1017 0.1020 \n",
            "\n",
            "[TRAIN] Epoch[4](17/20); Loss: 0.130020; Backpropagation: 0.3689 sec; Batch: 3.4722 sec\n",
            "0.2593 0.1908 0.1412 0.1229 0.1187 0.1148 0.1130 0.1135 0.1137 0.1134 0.1131 0.1139 0.1137 0.1128 0.1127 0.1127 \n",
            "\n",
            "[TRAIN] Epoch[4](18/20); Loss: 0.126328; Backpropagation: 0.3705 sec; Batch: 3.4703 sec\n",
            "0.2396 0.1767 0.1373 0.1225 0.1192 0.1149 0.1123 0.1117 0.1119 0.1113 0.1105 0.1102 0.1105 0.1100 0.1114 0.1113 \n",
            "\n",
            "[TRAIN] Epoch[4](19/20); Loss: 0.113608; Backpropagation: 0.3681 sec; Batch: 3.4597 sec\n",
            "0.2434 0.1735 0.1256 0.1063 0.1035 0.1001 0.0990 0.0988 0.0981 0.0968 0.0955 0.0952 0.0955 0.0957 0.0955 0.0955 \n",
            "\n",
            "[TRAIN] Epoch[4](20/20); Loss: 0.121472; Backpropagation: 0.3361 sec; Batch: 2.8421 sec\n",
            "0.2167 0.1626 0.1274 0.1147 0.1117 0.1103 0.1103 0.1101 0.1106 0.1108 0.1103 0.1099 0.1097 0.1092 0.1095 0.1096 \n",
            "\n",
            "[TRAIN] Epoch[5](1/20); Loss: 0.118814; Backpropagation: 0.3689 sec; Batch: 3.4731 sec\n",
            "0.2203 0.1595 0.1257 0.1149 0.1104 0.1083 0.1067 0.1060 0.1059 0.1057 0.1056 0.1062 0.1059 0.1060 0.1068 0.1071 \n",
            "\n",
            "[TRAIN] Epoch[5](2/20); Loss: 0.128319; Backpropagation: 0.3688 sec; Batch: 3.5146 sec\n",
            "0.2263 0.1769 0.1410 0.1251 0.1213 0.1175 0.1159 0.1152 0.1154 0.1151 0.1146 0.1141 0.1138 0.1135 0.1138 0.1137 \n",
            "\n",
            "[TRAIN] Epoch[5](3/20); Loss: 0.119434; Backpropagation: 0.3686 sec; Batch: 3.4668 sec\n",
            "0.2400 0.1726 0.1289 0.1133 0.1094 0.1060 0.1050 0.1051 0.1048 0.1035 0.1035 0.1041 0.1035 0.1031 0.1037 0.1046 \n",
            "\n",
            "[TRAIN] Epoch[5](4/20); Loss: 0.121699; Backpropagation: 0.3679 sec; Batch: 3.4699 sec\n",
            "0.2094 0.1530 0.1267 0.1208 0.1174 0.1159 0.1133 0.1124 0.1110 0.1098 0.1090 0.1095 0.1098 0.1103 0.1096 0.1091 \n",
            "\n",
            "[TRAIN] Epoch[5](5/20); Loss: 0.116826; Backpropagation: 0.3688 sec; Batch: 3.4757 sec\n",
            "0.2220 0.1584 0.1205 0.1139 0.1117 0.1087 0.1058 0.1040 0.1029 0.1034 0.1027 0.1024 0.1040 0.1041 0.1022 0.1026 \n",
            "\n",
            "[TRAIN] Epoch[5](6/20); Loss: 0.115848; Backpropagation: 0.3703 sec; Batch: 3.4719 sec\n",
            "0.2133 0.1505 0.1212 0.1153 0.1110 0.1084 0.1063 0.1048 0.1045 0.1034 0.1026 0.1021 0.1023 0.1015 0.1031 0.1034 \n",
            "\n",
            "[TRAIN] Epoch[5](7/20); Loss: 0.138255; Backpropagation: 0.3652 sec; Batch: 3.4688 sec\n",
            "0.2262 0.1738 0.1459 0.1369 0.1338 0.1307 0.1292 0.1291 0.1278 0.1266 0.1266 0.1255 0.1248 0.1253 0.1248 0.1250 \n",
            "\n",
            "[TRAIN] Epoch[5](8/20); Loss: 0.107004; Backpropagation: 0.3701 sec; Batch: 3.4690 sec\n",
            "0.2158 0.1519 0.1126 0.1047 0.1015 0.0965 0.0945 0.0942 0.0939 0.0930 0.0924 0.0922 0.0926 0.0915 0.0917 0.0930 \n",
            "\n",
            "[TRAIN] Epoch[5](9/20); Loss: 0.103690; Backpropagation: 0.3689 sec; Batch: 3.4750 sec\n",
            "0.2074 0.1369 0.1096 0.1032 0.0962 0.0948 0.0937 0.0930 0.0919 0.0910 0.0906 0.0906 0.0901 0.0899 0.0897 0.0904 \n",
            "\n",
            "[TRAIN] Epoch[5](10/20); Loss: 0.131199; Backpropagation: 0.3697 sec; Batch: 3.4731 sec\n",
            "0.2513 0.1794 0.1373 0.1295 0.1250 0.1190 0.1176 0.1167 0.1158 0.1153 0.1150 0.1154 0.1160 0.1153 0.1156 0.1151 \n",
            "\n",
            "[TRAIN] Epoch[5](11/20); Loss: 0.124316; Backpropagation: 0.3680 sec; Batch: 3.4664 sec\n",
            "0.2133 0.1547 0.1294 0.1234 0.1184 0.1175 0.1164 0.1140 0.1135 0.1135 0.1132 0.1125 0.1123 0.1121 0.1119 0.1131 \n",
            "\n",
            "[TRAIN] Epoch[5](12/20); Loss: 0.126930; Backpropagation: 0.3686 sec; Batch: 3.4663 sec\n",
            "0.2222 0.1574 0.1332 0.1279 0.1221 0.1200 0.1182 0.1168 0.1160 0.1146 0.1142 0.1144 0.1136 0.1136 0.1139 0.1129 \n",
            "\n",
            "[TRAIN] Epoch[5](13/20); Loss: 0.107200; Backpropagation: 0.4661 sec; Batch: 3.5620 sec\n",
            "0.2238 0.1537 0.1160 0.1059 0.1007 0.0968 0.0948 0.0938 0.0926 0.0916 0.0911 0.0911 0.0912 0.0909 0.0906 0.0906 \n",
            "\n",
            "[TRAIN] Epoch[5](14/20); Loss: 0.103104; Backpropagation: 0.3720 sec; Batch: 3.5232 sec\n",
            "0.1965 0.1304 0.1051 0.0980 0.0961 0.0954 0.0943 0.0936 0.0932 0.0927 0.0926 0.0921 0.0922 0.0925 0.0924 0.0926 \n",
            "\n",
            "[TRAIN] Epoch[5](15/20); Loss: 0.116405; Backpropagation: 0.3695 sec; Batch: 3.4700 sec\n",
            "0.2133 0.1485 0.1213 0.1148 0.1110 0.1087 0.1071 0.1064 0.1061 0.1049 0.1041 0.1038 0.1036 0.1034 0.1030 0.1025 \n",
            "\n",
            "[TRAIN] Epoch[5](16/20); Loss: 0.111369; Backpropagation: 0.3698 sec; Batch: 3.4764 sec\n",
            "0.2136 0.1516 0.1163 0.1063 0.1037 0.1022 0.1007 0.0996 0.0993 0.0989 0.0987 0.0983 0.0979 0.0974 0.0983 0.0990 \n",
            "\n",
            "[TRAIN] Epoch[5](17/20); Loss: 0.115047; Backpropagation: 0.3693 sec; Batch: 3.4734 sec\n",
            "0.1928 0.1421 0.1242 0.1156 0.1118 0.1095 0.1076 0.1064 0.1063 0.1048 0.1040 0.1038 0.1038 0.1031 0.1028 0.1021 \n",
            "\n",
            "[TRAIN] Epoch[5](18/20); Loss: 0.112240; Backpropagation: 0.4694 sec; Batch: 3.5665 sec\n",
            "0.2100 0.1453 0.1211 0.1128 0.1084 0.1062 0.1038 0.1018 0.0997 0.0990 0.0979 0.0979 0.0980 0.0981 0.0978 0.0979 \n",
            "\n",
            "[TRAIN] Epoch[5](19/20); Loss: 0.126065; Backpropagation: 0.3662 sec; Batch: 3.4622 sec\n",
            "0.2213 0.1636 0.1403 0.1301 0.1237 0.1186 0.1163 0.1145 0.1127 0.1121 0.1115 0.1110 0.1110 0.1102 0.1100 0.1102 \n",
            "\n",
            "[TRAIN] Epoch[5](20/20); Loss: 0.108281; Backpropagation: 0.3405 sec; Batch: 2.8500 sec\n",
            "0.2128 0.1467 0.1201 0.1061 0.1016 0.0986 0.0967 0.0962 0.0959 0.0951 0.0948 0.0938 0.0943 0.0934 0.0930 0.0934 \n",
            "\n",
            "[TRAIN] Epoch[6](1/20); Loss: 0.108051; Backpropagation: 0.3660 sec; Batch: 3.5203 sec\n",
            "0.2145 0.1432 0.1185 0.1069 0.1024 0.0996 0.0972 0.0958 0.0946 0.0941 0.0939 0.0933 0.0936 0.0941 0.0937 0.0936 \n",
            "\n",
            "[TRAIN] Epoch[6](2/20); Loss: 0.113274; Backpropagation: 0.3686 sec; Batch: 3.4657 sec\n",
            "0.2076 0.1467 0.1218 0.1129 0.1085 0.1066 0.1049 0.1040 0.1024 0.1014 0.1010 0.1001 0.0990 0.0986 0.0987 0.0982 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv2D6z4910qV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a44b6905-86db-4f33-8c28-52f05d2bf726"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U76QG-ym1yAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}